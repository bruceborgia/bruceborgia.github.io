---
layout: post
title: face_llm_note
tags: [note]
data: 2025-04-11 18:01 +0800
---
1. 为什么大模型大部分是decoder only
因为decoder-only结构模型在没有任何微调数据的情况下，zero-shot的表现能力最好。而encoder-decoder则
需要在一定量的标注数据上做multitask-finetuning才能够激发最佳性能。
目前的Large LM的训练范式还是在大规模语料shang 做自监督学习，很显然zero-shot性能更好的decoder-only架
构才能更好的利用这些无标注的数据。
大模型使用decoder-only架构除了训练效率和工程实现上的优势外，在理论上因为Encoder的双向注意力会存在
低秩的问题，这可能会削弱模型的表达能力。就生成任务而言，引入双向注意力并无实质的好处。而Encoder
decoder模型架构之所以能够在某些场景下表现更好，大概是因为它多了一倍参数。所以在同等参数量、同等推
理成本下，Decoder-only架构就是最优的选择了

2. 模型的泛化能力是什么，如何提高泛化能力
泛化能力：针对数据集中未出现的类型进行预测，有较好的预测能力
提高方法：
	1.增加数据多样性，避免数据单一
	2.使用正则化技术，dropout, bn
	3.早停，防止过拟合
	4. ood（out of distribution）加入一组与训练集数据有明显差异的数据用作验证集， 目的是评估和提升模型对未见过或异常数据的表现

3. 大模型如何解决幻觉问题
	1. 提升训练数据质量
	2. 检索增强生成
	3. 在提示词工程中明确指令只基于事实来回答，避免猜测
	4.自我改进
	5.多模型交叉验证

4.在模型实际部署过程中，如何解决多用户请求问题
	1.使用消息队列来缓冲高峰压力
	2.增加优先级调度
	3. 设置每秒请求上限
	4.将FP32转INT8
	5.对于常见的请求结果进行缓存
	6.使用分布式计算框架

5. GPT的原理
	1.预训练：基于大规模无标注数据进行训练，基于decoder-only，最小化交叉熵损失
	2.指令微调：从无监督向有监督转变微调，收集指令-响应对，比如”翻译i love you“输出：我爱你，这样的数据集进行微调，可以全参，也可以LoRA
	3.奖励模型的建立：训练一个独立的奖励模型，奖励模型是一个分类器或者回归模型，输入时”输入-输出对“，输出是一个评分，例如：回答“1+1=3”得低分，“1+1=2”得高分。
	4.对齐：基于人类反馈的强化学习（RLHF, Reinforcement Learning from Human Feedback）,让模型不仅能生成正确的内容，还能符合伦理规范、用户期待和社会价值观。

6. self-attention公式是什么，按照公式计算有什么问题
	softmax(qk/根号dk)v
	问题：出现数值溢出，解决方法：减去一个max值，既safe-softmax
7. 如何降低softmaxGPU访存复杂度

8.llama如何优化注意力机制的计算的
https://zhuanlan.zhihu.com/p/636784644
llama1 使用多头注意力机制(MHA), llama2,llama3 使用分组查询注意力机制(GQA)
自回归模型生成回答时，需要前面生成的KV缓存起来，来加速计算。多头注意力机制(MHA)需要的缓存量很大，,GQA没有像MQA一样极端，将query分组，组内共享KV，效果接近MHA，速度上与MQA可比

9.layer norm对那个维度做标准化
输入维度为[batch, seqlen, D],layer针对每个单词的embedding做标准化，因此在seqlen

10.RAG如何做效果评估
	rag主要做检索和生成环节，
		检索环节：
			1.召回率：topk检索结果中相关文档的比例
			2.精确率：检索结果中相关文档的比例
			3.平均倒排率：MRR：关注第一个相关文档的排名
		生成环节：1.事实准确性：检查回答是否文档一致，是否覆盖
			2.关键词重合度
			3.文本相似度
11、如何优化rag
	1.优化模型，使用更加大的模型
	2.优化提示词：加入 不得虚构，若信息不足则说无法确定
	3.若高recall，检索环节可靠，问题在生成，低recall，优化检索模型和知识库
	4.让大模型对结果进行反思
12.rag的幻觉问题：
	1.生成结果和数据源不一致
	2.问题超出大模型的认知
13.rag的步骤
	1.分块
	2.用编码模型把这些块嵌入到向量中，把向量建立索引
	3.给大模型建立一个提示：根据搜索结果回答用户问题
	运行时：使用相同的编码器对用户的查询进行向量化，针对索引执行查询向量的搜索，根据结果输入给大模型

14.transformer中为什么对KV进行缓存
transformer中文本是逐个生成的，需要使用之前的每个KV，这样可以不用计算，直接读取

15.在 Transformer 模型（特别是大语言模型）的推理过程中，Prefill 和 Decoding 是自回归生成（Autoregressive Generation）的两个关键阶段
	prefill阶段：Prefill（预填充）是指在生成任务开始前，模型一次性处理完整的输入序列（Prompt），计算所有 token 的 Key（K）和 Value（V）向量，并将其存入 KV 缓存，为后续生成准备上下文。
	decoding：Decoding（解码）是指在 Prefill 完成后，模型逐个生成新 token 的过程，利用 KV 缓存逐步扩展序列。

16.KV缓存有什么问题
	超长序列时，kv缓存占用显存过大

17.如何解决kv缓存占用过大问题
	量化kv

18.大模型服务吞吐率太小怎么办
	1. 降低模型的推理延迟
	2. 增大模型并行处理请求能力
19.如何提高
	1.如果GPU还有空余，可以使用投机采样：小模型猜测加大模型验证
	2. 使用批量处理，将多个用户请求合成一个批次处理
	3.使用异步处理
	4.部署多个模型实例，通过队列分发
	5.使用模型量化